install.packages("blsAPI")
install.packages("jsonlite")
install.packages("rtimes")
knitr::opts_chunk$set(echo = TRUE)
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
ls()
head(trump_tweets_df)
View(trump_tweets_df)
sum(grepl("#", trump_tweets_df$text))
print(gsub("\\D+", "",trump_tweets_df$text ), quote = FALSE)
China <- grep("China",trump_tweets_df$text, ignore.case = TRUE, value = TRUE)
gsub("China", replacement = "USA", China, ignore.case = TRUE)
gsub("China", replacement = "USA", China)
library(jsonlite)
library(blsAPI)
layoffs_json <- blsAPI('MLUMS00NS0022003') #Electronic and other electrical equipment
class(layoffs_json)
json_file <- fromJSON(layoffs_json)
class(json_file)
layoffs_json <- blsAPI('MLUMS00NN0007003') #Manufact N0007
class(layoffs_json)
json_file <- fromJSON(layoffs_json)
class(json_file)
data_from_api<-json_file$Results$series$data
data_from_api<-as.data.frame(data_from_api) #convert list to data.frame
data_from_api #print results
class(data_from_api) #now it's a typical data frame you can analyze in R
data_from_api #print results
View(data_from_api)
data_from_api$year <-as.numeric(data_from_api$year)
data_from_api$value<-as.numeric(data_from_api$value)
View(data_from_api)
library(dplyr)
data_from_api_1 <- data_from_api %>% filter(periodName!=Annual)
data_from_api_1 <- data_from_api %>% filter(periodName!= "Annual")
View(data_from_api_1)
library(ggplot2)
data_from_api_1 <- data_from_api %>% filter(periodName!= "Annual")%>%
mutate(timer=year & "-" &period)
data_from_api_1 <- data_from_api %>% filter(periodName!= "Annual")%>%
mutate(timer=year&period)
data_from_api_1 <- data_from_api %>% filter(periodName!= "Annual")%>%
mutate(timer=year&&period)
data_from_api_1 <- data_from_api %>% filter(periodName!= "Annual")%>%
mutate(timer= paste(year,period,sep="_"))
ggplot(data=data_from_api_1, mapping = aes(x=timer,fill=value)) +
geom_bar(alpha = 1/5, position = "identity")
ggplot(data=data_from_api_1, mapping = aes(x=year,fill=value)) +
geom_bar(alpha = 1/5, position = "identity")
ggplot(data=data_from_api_1, mapping = aes(x=timer,fill=value)) +
geom_bar(alpha = 1/5, position = "identity")
ggplot(data=data_from_api_1, mapping = aes(x=timer,fill=value)) +
geom_bar(alpha = 1/5, position = "dodge")
ggplot(data=data_from_api_1, mapping = aes(x=timer,fill=value)) +
geom_bar(alpha = 1/5, position = "dodge")
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value)) +
geom_point(mapping=aes(color=year))
ggplot(data=data_from_api_1, mapping = aes(x=year,y=value)) +
geom_point(mapping=aes(color=year))
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value)) +
geom_point(mapping=aes(color=year))
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value)) +
geom_line(mapping=aes(color=year))
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value)) +
geom_line(mapping=aes(y=value))
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value)) +
geom_point(mapping=aes(color=year))
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value)) +
geom_bar(mapping=aes(x=year))
ggplot(data=data_from_api_1) +
geom_bar(mapping=aes(x=year))
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value))+
geom_point(color=year)
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value))+
geom_point(fill=year)
ggplot(data=data_from_api_1)+
geom_point(mapping = aes(x=timer,y=value))
ggplot(data=data_from_api_1)+
geom_point(mapping = aes(x=timer,y=value,fill=year))
ggplot(data=data_from_api_1)+
geom_point(mapping = aes(x=timer,y=value,fill=year))
ggplot(data=data_from_api_1, mapping = aes(x=timer,fill=value))+
geom_point(mapping = aes(fill=year)
#It seems that 2012 and 2011 have the same vlaue but it decreased in 2013.
ggplot(data=data_from_api_1, mapping = aes(x=timer,fill=value))+
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value))+
geom_point(mapping = aes(fill=year)
#It seems that 2012 and 2011 have the same vlaue but it decreased in 2013.
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value))+
library(ggplot2)
ggplot(data=data_from_api_1) +
geom_bar(mapping=aes(x=year))
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value))+
geom_point(mapping = aes(fill=year))
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value))+
geom_point(mapping = aes(color=year))
layoffs_json_ny <- blsAPI('MLUMS36NN0007003')  #New York State code: S36
json_file_ny <- fromJSON(layoffs_json_ny)
data_from_api_ny<-json_file1$Results$series$data
data_from_api_ny<-json_file_ny$Results$series$data
data_from_api_ny<-as.data.frame(data_from_api_ny)
View(data_from_api_ny)
data_from_api_ny$year <-as.numeric(data_from_api_ny$year)
data_from_api_ny$value<-as.numeric(data_from_api_ny$value)
data_from_api_ny_1 <- data_from_api %>% filter(periodName!= "Annual")%>%
mutate(timer= paste(year,period,sep="_"))
ggplot(data=data_from_api_ny_1) +
geom_bar(mapping=aes(x=year))
ggplot(data=data_from_api_ny_1, mapping = aes(x=timer,y=value))+
geom_point(mapping = aes(color=year))
View(data_from_api_ny_1)
data_from_api_ny_1 <- data_from_api_ny %>% filter(periodName!= "Annual")%>%
mutate(timer= paste(year,period,sep="_"))
ggplot(data=data_from_api_ny_1) +
geom_bar(mapping=aes(x=year))
ggplot(data=data_from_api_ny_1, mapping = aes(x=timer,y=value))+
geom_point(mapping = aes(color=year))
install.packages("twitteR")
library("twitteR", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library(twitteR)
consumer_key <- "9N66oSyD2HfZmADFzrx5BpcYC"
consumer_secret <- "ftvpJwbVxCVlVCYcczICwmaNG8T0cNrBkaSBjJiPoWkdOHygYc"
access_token <- "1048293511962025984-Ix7hJIWZ5RqplEzMtgie87i48vvk0E"
access_secret <- "QHBCqJ5bi9nxMmn2E2rsQQ09CFlKT4IM4mtTWtIgcBPxH"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
tw_Trump <- twitteR::searchTwitter('#realDonaldTrump', n = 1e4, since = '2018-01-01', retryOnRateLimit = 1e3)
result <- twitteR::twListToDF(tw_Trump)
View(result)
head(result)
article_key <- "zcOzAIPXd3FNW9yaZFz0f2mEJrips0GT"
term <- "Hillary Clinton"
begin_date <-"20170101"
end_date <- "20190101"
finalurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&api-key=",article_key, sep="")
mydata <- fromJSON(finalurl)
names(mydata) #the list names are different when data is retrieved directly from the API
mydata$response #response = the data you want
mydata<-as.data.frame(mydata$response) #extract final data to data.frame
View(mydata)
head(mydata)
View(mydata)
View(mydata)
View(mydata)
View(mydata)
nrow(as.data.frame(mydata$docs._id))
ggplot(data=data_from_api_1) +
geom_bar(mapping=aes(x=value))
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value))+
geom_point(mapping = aes(color=year))+
theme(text = element_text(size=20),
axis.text.x = element_text(angle=90, hjust=1))
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value))+
geom_point(mapping = aes(color=year))+
theme(text = element_text(size=5),
axis.text.x = element_text(angle=90, hjust=1))
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value))+
geom_point(mapping = aes(color=year))+
theme(text = element_text(size=8),
axis.text.x = element_text(angle=90, hjust=1))
ggplot(data=data_from_api_ny_1, mapping = aes(x=timer,y=value))+
geom_point(mapping = aes(color=year))+
theme(text = element_text(size=8),
axis.text.x = element_text(angle=90, hjust=1))
head(gsub("\\D+", "",trump_tweets_df$text ), quote = FALSE)
head(gsub("\\D+", "",trump_tweets_df$text ), quote = FALSE,10)
head(data_from_api_ny)
head(data_from_api_ny,10)
head(data_from_api,10) #print results
head(mydata,10)
head(result,10)
head(trump_tweets_df,10)
library(jsonlite)
library(blsAPI)
library(dplyr)
library(ggplot2)
layoffs_json <- blsAPI('MLUMS00NN0007003') #Manufacture code: N0007
class(layoffs_json)
json_file <- fromJSON(layoffs_json)
class(json_file)
# data is nested within json_file > Results > series > then data
data_from_api<-json_file$Results$series$data
data_from_api<-as.data.frame(data_from_api) #convert list to data.frame
head(data_from_api,10) #print results
class(data_from_api) #now it's a typical data frame you can analyze in R
#ggplot the value based on year
data_from_api$year <-as.numeric(data_from_api$year)
data_from_api$value<-as.numeric(data_from_api$value)
data_from_api_1 <- data_from_api %>% filter(periodName!= "Annual")%>%
mutate(timer= paste(year,period,sep="_"))
ggplot(data=data_from_api_1, mapping = aes(x=timer,y=value))+
geom_point(mapping = aes(color=year))+
theme(text = element_text(size=8),
axis.text.x = element_text(angle=90, hjust=1))
#It seems that 2012 and 2011 have the same vlaue but it decreased in 2013.
mail_lists=readLines("http://www.r-project.org/mail.html")
mail_lists=readLines("http://www.r-project.org/mail.html")
fix(mail_lists)
mail_lists_http <- grep("http", mail_lists,value=TRUE)
fix(mail_lists_http)
mail_lists_split <-strsplit(mail_lists_http,split="http",fixed=TRUE)
fix(mail_lists_split)
class(mail_lists_split)
class(mail_lists_http)
mail_lists_secElem <- unlist(lapply(mail_lists_split,function(x) x[[2]]))
class(mail_lists_secElem)
fix(mail_lists_secElem)
mail_lists_split_2 <-strsplit(mail_lists_secElem, split = ">", fixed=TRUE)
class(mail_lists_split_2)
fix(mail_lists_split_2)
fix(mail_lists_secElem)
fix(mail_lists_split_2)
jsoncars <- toJSON(mtcars, pretty=TRUE)
cat(jsoncars)
fromJSON(jsoncars)
class(fromJSON(jsoncars))
url <- "https://data.cityofnewyork.us/api/views/kku6-nxdu/rows.csv"
data_gov <- read.csv (url, stringsAsFactors = FALSE)
data_gov[1:6, c (1,3:4)]
library (gdata)
url <- "http://www.huduser.org/portal/datasets/fmr/fmr2015f/FY2015F_4050_Final.xls"
rents <- read.xls (url)
install.packages("data")
install.packages("gdata")
library (gdata)
library (gdata)
url <- "http://www.huduser.org/portal/datasets/fmr/fmr2015f/FY2015F_4050_Final.xls"
rents <- read.xls (url)
rents <- read.xls (url, perl="C:/Strawberry/perl/bin/perl.exe")
rents[1:6, 1:10]
sheetCount(url, perl="C:/Strawberry/perl/bin/perl.exe")
sheetNames(url, perl="C:/Strawberry/perl/bin/perl.exe")
url <- "http://www.bls.gov/cex/pumd/data/comma/diary14.zip"
download.file (url, dest="dataset.zip")
unzip ("dataset.zip")
list.files ("diary14")
zip_data <- read.csv ( unz ("dataset.zip", "diary14/expd141.csv"))
zip_data[1:5, 1:10]
temp <- tempfile ()
download.file ("http://www.bls.gov/cex/pumd/data/comma/diary14.zip",temp)
zip_data2 <- read.csv ( unz (temp, "diary14/expd141.csv"))
unlink (temp)
zip_data2[1:5, 1:10]
library (rvest)
page <- read_html("https://en.wikipedia.org/wiki/Web_scraping")
page %>%
html_nodes("*") %>%
html_name() %>% #extracts all html tags
unique()
scraping_wiki <- read_html ("https://en.wikipedia.org/wiki/Web_scraping")
scraping_wiki %>%
html_nodes ("h1")
scraping_wiki <- read_html ("https://en.wikipedia.org/wiki/Web_scraping")
scraping_wiki %>%
html_nodes ("h1") %>%
html_text()
scraping_wiki %>%
html_nodes ("h2") %>%
html_text ()
p_nodes <- scraping_wiki %>%
html_nodes ("p")
length (p_nodes)
p_nodes[1:6]
p_text <- scraping_wiki %>%
html_nodes ("p") %>%
html_text ()
ul_text <- scraping_wiki %>%
html_nodes ("ul") %>%
html_text ()
ul_text[1] #text from table of contents list near beginning of page
substr (ul_text[2], start = 1, stop = 200) #first 200 characters
substr (ul_text[2], start = 1, stop = 10) #first 10 characters
all_text <- scraping_wiki %>%
html_nodes ("div") %>%
html_text ()
links<-scraping_wiki %>%
html_nodes("a") %>%
html_attr("href")
grep("http", links, value=TRUE) #One of many ways to extract links you want from page
body_text <- scraping_wiki %>%
html_nodes ("#mw-content-text") %>%
html_text ()
substr (body_text, start = 1, stop = 207)
substr (body_text, start = nchar (body_text)-73, stop = nchar (body_text))
scraping_wiki %>%
html_nodes ("#Techniques") %>%
html_text ()
paragraphs<-scraping_wiki %>%
html_nodes ("#mw-content-text") %>%
html_nodes ("p") %>%
html_text ()
paragraphs[3] #extract third paragraph
grep("machine learning",paragraphs) #find paragraph in vector with specific text
links
