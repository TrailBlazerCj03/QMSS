{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1.  Import the spam dataset and print the first six rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "0             0.00                0.64            0.64            0.0   \n",
       "1             0.21                0.28            0.50            0.0   \n",
       "2             0.06                0.00            0.71            0.0   \n",
       "3             0.00                0.00            0.00            0.0   \n",
       "4             0.00                0.00            0.00            0.0   \n",
       "5             0.00                0.00            0.00            0.0   \n",
       "\n",
       "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
       "0            0.32             0.00               0.00                 0.00   \n",
       "1            0.14             0.28               0.21                 0.07   \n",
       "2            1.23             0.19               0.19                 0.12   \n",
       "3            0.63             0.00               0.31                 0.63   \n",
       "4            0.63             0.00               0.31                 0.63   \n",
       "5            1.85             0.00               0.00                 1.85   \n",
       "\n",
       "   word_freq_order:  word_freq_mail:  ...   char_freq_;:  char_freq_(:  \\\n",
       "0              0.00             0.00  ...           0.00         0.000   \n",
       "1              0.00             0.94  ...           0.00         0.132   \n",
       "2              0.64             0.25  ...           0.01         0.143   \n",
       "3              0.31             0.63  ...           0.00         0.137   \n",
       "4              0.31             0.63  ...           0.00         0.135   \n",
       "5              0.00             0.00  ...           0.00         0.223   \n",
       "\n",
       "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "0           0.0         0.778         0.000         0.000   \n",
       "1           0.0         0.372         0.180         0.048   \n",
       "2           0.0         0.276         0.184         0.010   \n",
       "3           0.0         0.137         0.000         0.000   \n",
       "4           0.0         0.135         0.000         0.000   \n",
       "5           0.0         0.000         0.000         0.000   \n",
       "\n",
       "   capital_run_length_average:  capital_run_length_longest:  \\\n",
       "0                        3.756                           61   \n",
       "1                        5.114                          101   \n",
       "2                        9.821                          485   \n",
       "3                        3.537                           40   \n",
       "4                        3.537                           40   \n",
       "5                        3.000                           15   \n",
       "\n",
       "   capital_run_length_total:  spam  \n",
       "0                        278     1  \n",
       "1                       1028     1  \n",
       "2                       2259     1  \n",
       "3                        191     1  \n",
       "4                        191     1  \n",
       "5                         54     1  \n",
       "\n",
       "[6 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir=\"./Mid_Term_Data/spam_dataset.csv\"\n",
    "df=pd.read_csv(dataDir)\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 58)\n",
      "(4601, 58)\n"
     ]
    }
   ],
   "source": [
    "#make sure to drop all NA value\n",
    "print(df.shape)\n",
    "df_1=df.dropna()\n",
    "print(df_1.shape)\n",
    "#SInce there is no difference, 'df' will continue to be used in the next for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. The dependent variable is \"spam\" where one indicates that an email is spam and zero otherwise.  Which three variables in the dataset do you think will be important predictors in a model of spam?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my perspective, word_freq_business, word_freq_conference, char_freq_! is the three most important predictors in a model of spam. Usually spam emails are commercial emails and they usually ask for collaboration chances in business or ask you to attend some strange conferences. And to stress the significance of themselves, ! is the most frequent symbol in the email according to my experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Visualize the univariate distribution of each of the variables in the previous question.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: spam, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df[\"spam\"]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.00\n",
      "1    0.07\n",
      "2    0.06\n",
      "3    0.00\n",
      "4    0.00\n",
      "Name: word_freq_business:, dtype: float64\n",
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "Name: word_freq_conference:, dtype: float64\n",
      "0    0.778\n",
      "1    0.372\n",
      "2    0.276\n",
      "3    0.137\n",
      "4    0.135\n",
      "Name: char_freq_!:, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X1=df.iloc[:, 16]   #business\n",
    "X2=df.iloc[:, 47] #conference\n",
    "X3=df.iloc[:,51] #CHAR_!\n",
    "\n",
    "\n",
    "print (X1.head())\n",
    "print(X2.head())\n",
    "print(X3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_business:</th>\n",
       "      <th>word_freq_conference:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_business:  word_freq_conference:  char_freq_!:\n",
       "0                 0.00                    0.0         0.778\n",
       "1                 0.07                    0.0         0.372\n",
       "2                 0.06                    0.0         0.276\n",
       "3                 0.00                    0.0         0.137\n",
       "4                 0.00                    0.0         0.135"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df.iloc[:,[16,47, 51]]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(color_codes=True)\n",
    "sns.distplot(X1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(X2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(X3,);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(y,kde=False, rug=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. Name each of the supervised learning models that we have learned thus far that are used to predict dependent variables like \"spam\".   (Include models we have learned through class 5 only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. K-Nearest Neighborhood;\n",
    "2. Linear Regression;\n",
    "3. Logistic Regression;\n",
    "4. Support Vector Machine;\n",
    "5. Decision Tree;\n",
    "6. Random Forest;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5. Describe the importance of training and test data.  Why do we separate data into these subsets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, it aims to avoid underfitting, overfitting and some other problems. \n",
    "\n",
    "Detailedly, training set and test set are used in distinct purposes. When training a model, it tries to find out some pattern in the  training data while minimizing the error rate.  After finishing trained the model, we should have a test it on fresh examples in order to avoid problems like overfitting. The purpose of the training dataset is to provide you with some algorithm. Test dataset, however, is used to assess how well your algorithm was trained with the training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6. What is k-fold cross validation and what do we use it for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. \n",
    "\n",
    "Steps of k-fold:\n",
    "    a. Divide the data into K roughly equal parts, (ususally K=5 or 10);\n",
    "    b. for each k = 1, 2, . . . K, fit the model with parameter λ to the other K − 1 parts, giving $beta^(-k) (lamda)$ and compute its error in\n",
    "predicting the kth part Ek(lamda),\n",
    "    This gives the cross-validation error:\n",
    "    $$CV(lamda)=\\frac{1}{K}\\sum_{k=1}^K Ek(lamda)$$\n",
    "    c. do this for many values of λ and choose the value of λ that makes CV (λ) smallest.\n",
    "\n",
    "The fitting process optimizes the model parameters to make the model fit the training data as well as possible.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7. How is k-fold cross validation different from stratified k-fold cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified means ensure relative class frequency in each fold reflect relative class frequencies on the whole dataset. Generally this is done in a supervised way for classification and aims to ensure each class is (approximately) equally represented across each test fold (which are of course combined in a complementary way to form training folds). Simply k-fold can be used in an unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#8. Choose one model from question four.  Split the data into training and test subsets.  Build a model with the three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 2.13495466 -4.30445844  1.97517449]]\n",
      "Training set score: 0.755\n",
      "Test set score: 0.757\n",
      "logreg.predict: [1 0 0 ... 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1e+90, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic model is chosen.\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, random_state=621) \n",
    "logreg = LogisticRegression(C=1e90).fit(X_train_1, y_train_1)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train_1, y_train_1)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test_1, y_test_1)))\n",
    "\n",
    "predicted_vals = logreg.predict(X_test_1) # y_pred includes your predictions\n",
    "print(\"logreg.predict: {}\".format(predicted_vals))\n",
    "logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold:\n",
      "[0.20955483 0.28369565 0.87717391 0.90108696 0.8326087 ]\n",
      "StratifiedKFold:\n",
      "[0.76764387 0.74809989 0.76221498 0.75952122 0.74755169]\n",
      "RepeatedKFold:\n",
      "[0.76655809 0.74130435 0.7576087  0.76086957 0.77282609 0.76438654\n",
      " 0.75869565 0.74347826 0.75543478 0.77608696 0.76221498 0.74130435\n",
      " 0.74673913 0.79347826 0.74782609 0.77198697 0.75543478 0.76521739\n",
      " 0.75108696 0.75652174 0.76764387 0.70869565 0.76847826 0.75652174\n",
      " 0.77826087 0.75895765 0.76195652 0.75108696 0.75978261 0.76195652\n",
      " 0.74592834 0.75978261 0.77608696 0.74130435 0.76956522 0.73724213\n",
      " 0.775      0.76847826 0.74347826 0.76521739 0.7339848  0.7826087\n",
      " 0.74347826 0.76413043 0.76413043 0.75570033 0.79782609 0.73043478\n",
      " 0.7576087  0.7576087 ]\n"
     ]
    }
   ],
   "source": [
    "#import cross validation functions from sk learn\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# K-Fold Cross Validatio\n",
    "kfold = KFold(n_splits=5)\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=10)\n",
    "print(\"KFold:\\n{}\".format(\n",
    "cross_val_score(LogisticRegression(), X, y, cv=kfold)))\n",
    "\n",
    "print(\"StratifiedKFold:\\n{}\".format(\n",
    "cross_val_score(LogisticRegression(), X, y, cv=skfold)))\n",
    "\n",
    "print(\"RepeatedKFold:\\n{}\".format(\n",
    "cross_val_score(LogisticRegression(), X, y, cv=rkf)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/statsmodels/genmod/families/family.py:880: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  n_endog_mu = self._clean((1. - endog) / (1. - mu))\n",
      "/anaconda3/lib/python3.6/site-packages/statsmodels/genmod/families/family.py:932: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  special.gammaln(n - y + 1) + y * np.log(mu / (1 - mu)) +\n",
      "/anaconda3/lib/python3.6/site-packages/statsmodels/genmod/families/family.py:932: RuntimeWarning: invalid value encountered in multiply\n",
      "  special.gammaln(n - y + 1) + y * np.log(mu / (1 - mu)) +\n",
      "/anaconda3/lib/python3.6/site-packages/statsmodels/genmod/families/family.py:933: RuntimeWarning: divide by zero encountered in log\n",
      "  n * np.log(1 - mu)) * var_weights\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>spam</td>       <th>  No. Observations:  </th>  <td>  3450</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  3446</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>     3</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td>     nan</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Sat, 22 Jun 2019</td> <th>  Deviance:          </th> <td>     inf</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>00:02:35</td>     <th>  Pearson chi2:      </th> <td>5.63e+15</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>         <td>3</td>        <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>               <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                 <td>   -1.0391</td> <td>    0.046</td> <td>  -22.429</td> <td> 0.000</td> <td>   -1.130</td> <td>   -0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_business:</th>   <td>    2.0978</td> <td>    0.165</td> <td>   12.725</td> <td> 0.000</td> <td>    1.775</td> <td>    2.421</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_conference:</th> <td>   -2.6920</td> <td>    0.461</td> <td>   -5.841</td> <td> 0.000</td> <td>   -3.595</td> <td>   -1.789</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>char_freq_!:</th>          <td>    1.8693</td> <td>    0.113</td> <td>   16.605</td> <td> 0.000</td> <td>    1.649</td> <td>    2.090</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                   spam   No. Observations:                 3450\n",
       "Model:                            GLM   Df Residuals:                     3446\n",
       "Model Family:                Binomial   Df Model:                            3\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                    nan\n",
       "Date:                Sat, 22 Jun 2019   Deviance:                          inf\n",
       "Time:                        00:02:35   Pearson chi2:                 5.63e+15\n",
       "No. Iterations:                     3   Covariance Type:             nonrobust\n",
       "=========================================================================================\n",
       "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------\n",
       "const                    -1.0391      0.046    -22.429      0.000      -1.130      -0.948\n",
       "word_freq_business:       2.0978      0.165     12.725      0.000       1.775       2.421\n",
       "word_freq_conference:    -2.6920      0.461     -5.841      0.000      -3.595      -1.789\n",
       "char_freq_!:              1.8693      0.113     16.605      0.000       1.649       2.090\n",
       "=========================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_train_1_new = sm.add_constant(X_train_1)\n",
    "model = sm.GLM(y_train_1, X_train_1_new, family=sm.families.Binomial()).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#9. Choose a second model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "#KNN is selected\n",
    "#(K=5)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_2, y_train_2)\n",
    "\n",
    "#Print accuracy rounded to two digits to the right of decimal\n",
    "print(\"accuracy: {:.2f}\".format(knn.score(X_test_2, y_test_2)))\n",
    "\n",
    "y_pred = knn.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "#K=10\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train_2, y_train_2)\n",
    "\n",
    "#Print accuracy rounded to two digits to the right of decimal\n",
    "print(\"accuracy: {:.2f}\".format(knn.score(X_test_2, y_test_2)))\n",
    "\n",
    "y_pred = knn.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred # view predictions for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold:\n",
      "[0.56786102 0.58478261 0.83369565 0.8923913  0.14891304]\n",
      "StratifiedKFold:\n",
      "[0.80998914 0.80238871 0.80456026 0.80195865 0.80413493]\n",
      "RepeatedKFold:\n",
      "[0.79153094 0.79782609 0.81304348 0.80217391 0.79347826 0.78284473\n",
      " 0.80543478 0.8076087  0.82717391 0.79021739 0.79261672 0.81304348\n",
      " 0.79673913 0.80978261 0.78695652 0.79587405 0.80326087 0.80978261\n",
      " 0.79891304 0.82826087 0.78935939 0.80434783 0.80217391 0.81630435\n",
      " 0.80108696 0.82084691 0.79673913 0.79891304 0.79021739 0.80108696\n",
      " 0.82410423 0.80434783 0.8        0.79021739 0.80543478 0.77415852\n",
      " 0.82065217 0.8        0.80217391 0.78043478 0.7980456  0.78152174\n",
      " 0.80978261 0.79565217 0.83369565 0.79587405 0.79130435 0.80217391\n",
      " 0.81195652 0.82065217]\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "print(\"KFold:\\n{}\".format(\n",
    "cross_val_score(KNeighborsClassifier(), X, y, cv=kfold)))\n",
    "\n",
    "print(\"StratifiedKFold:\\n{}\".format(\n",
    "cross_val_score(KNeighborsClassifier(n_neighbors=10), X, y, cv=skfold)))\n",
    "print(\"RepeatedKFold:\\n{}\".format(\n",
    "cross_val_score(KNeighborsClassifier(n_neighbors=10), X, y, cv=rkf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.798\n",
      "best parameters: {'n_neighbors': 11}\n",
      "test-set score: 0.817\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, stratify=y)\n",
    "\n",
    "#create dictionary data object with keys equal to parameter name 'n_neighbors' \n",
    "#for knn model and values equal to range of k values to create models for\n",
    "\n",
    "param_grid = {'n_neighbors': np.arange(1, 15, 2)} #np.arange creates sequence of numbers for each k value\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10)\n",
    "\n",
    "#use meta model methods to fit score and predict model:\n",
    "grid.fit(X_train_2, y_train_2)\n",
    "\n",
    "#extract best score and parameter by calling objects \"best_score_\" and \"best_params_\"\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid.score(X_test_2, y_test_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_neighbors': 1}</td>\n",
       "      <td>0.806358</td>\n",
       "      <td>0.773913</td>\n",
       "      <td>0.817391</td>\n",
       "      <td>0.791304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892754</td>\n",
       "      <td>0.897262</td>\n",
       "      <td>0.891787</td>\n",
       "      <td>0.895330</td>\n",
       "      <td>0.895008</td>\n",
       "      <td>0.895008</td>\n",
       "      <td>0.892110</td>\n",
       "      <td>0.889569</td>\n",
       "      <td>0.893945</td>\n",
       "      <td>0.002184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005230</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_neighbors': 3}</td>\n",
       "      <td>0.794798</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.814493</td>\n",
       "      <td>0.469565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859903</td>\n",
       "      <td>0.523027</td>\n",
       "      <td>0.523671</td>\n",
       "      <td>0.864734</td>\n",
       "      <td>0.527536</td>\n",
       "      <td>0.861192</td>\n",
       "      <td>0.859903</td>\n",
       "      <td>0.865422</td>\n",
       "      <td>0.760612</td>\n",
       "      <td>0.154428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005154</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_neighbors': 5}</td>\n",
       "      <td>0.797688</td>\n",
       "      <td>0.817391</td>\n",
       "      <td>0.805797</td>\n",
       "      <td>0.469565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841224</td>\n",
       "      <td>0.505636</td>\n",
       "      <td>0.503382</td>\n",
       "      <td>0.846699</td>\n",
       "      <td>0.840258</td>\n",
       "      <td>0.839936</td>\n",
       "      <td>0.840902</td>\n",
       "      <td>0.841275</td>\n",
       "      <td>0.774493</td>\n",
       "      <td>0.135009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>7</td>\n",
       "      <td>{'n_neighbors': 7}</td>\n",
       "      <td>0.803468</td>\n",
       "      <td>0.808696</td>\n",
       "      <td>0.808696</td>\n",
       "      <td>0.481159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833494</td>\n",
       "      <td>0.493398</td>\n",
       "      <td>0.494686</td>\n",
       "      <td>0.836715</td>\n",
       "      <td>0.829952</td>\n",
       "      <td>0.832850</td>\n",
       "      <td>0.832206</td>\n",
       "      <td>0.831294</td>\n",
       "      <td>0.765250</td>\n",
       "      <td>0.135620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>9</td>\n",
       "      <td>{'n_neighbors': 9}</td>\n",
       "      <td>0.797688</td>\n",
       "      <td>0.805797</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.830596</td>\n",
       "      <td>0.827697</td>\n",
       "      <td>0.828986</td>\n",
       "      <td>0.820934</td>\n",
       "      <td>0.828019</td>\n",
       "      <td>0.826731</td>\n",
       "      <td>0.830650</td>\n",
       "      <td>0.826538</td>\n",
       "      <td>0.003276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005036</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>11</td>\n",
       "      <td>{'n_neighbors': 11}</td>\n",
       "      <td>0.794798</td>\n",
       "      <td>0.814493</td>\n",
       "      <td>0.814493</td>\n",
       "      <td>0.768116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821578</td>\n",
       "      <td>0.827375</td>\n",
       "      <td>0.818680</td>\n",
       "      <td>0.828019</td>\n",
       "      <td>0.819002</td>\n",
       "      <td>0.820612</td>\n",
       "      <td>0.820934</td>\n",
       "      <td>0.822923</td>\n",
       "      <td>0.821481</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005253</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>13</td>\n",
       "      <td>{'n_neighbors': 13}</td>\n",
       "      <td>0.789017</td>\n",
       "      <td>0.817391</td>\n",
       "      <td>0.805797</td>\n",
       "      <td>0.762319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813527</td>\n",
       "      <td>0.821256</td>\n",
       "      <td>0.817713</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.817713</td>\n",
       "      <td>0.816747</td>\n",
       "      <td>0.815781</td>\n",
       "      <td>0.819704</td>\n",
       "      <td>0.817810</td>\n",
       "      <td>0.003597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.005097      0.000254         0.002500        0.000184   \n",
       "1       0.005230      0.000393         0.002776        0.000272   \n",
       "2       0.005154      0.000242         0.002780        0.000243   \n",
       "3       0.005200      0.000222         0.002850        0.000158   \n",
       "4       0.005126      0.000314         0.002908        0.000214   \n",
       "5       0.005036      0.000151         0.002911        0.000172   \n",
       "6       0.005253      0.000314         0.003220        0.000598   \n",
       "\n",
       "  param_n_neighbors               params  split0_test_score  \\\n",
       "0                 1   {'n_neighbors': 1}           0.806358   \n",
       "1                 3   {'n_neighbors': 3}           0.794798   \n",
       "2                 5   {'n_neighbors': 5}           0.797688   \n",
       "3                 7   {'n_neighbors': 7}           0.803468   \n",
       "4                 9   {'n_neighbors': 9}           0.797688   \n",
       "5                11  {'n_neighbors': 11}           0.794798   \n",
       "6                13  {'n_neighbors': 13}           0.789017   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score       ...         \\\n",
       "0           0.773913           0.817391           0.791304       ...          \n",
       "1           0.811594           0.814493           0.469565       ...          \n",
       "2           0.817391           0.805797           0.469565       ...          \n",
       "3           0.808696           0.808696           0.481159       ...          \n",
       "4           0.805797           0.811594           0.765217       ...          \n",
       "5           0.814493           0.814493           0.768116       ...          \n",
       "6           0.817391           0.805797           0.762319       ...          \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0            0.892754            0.897262            0.891787   \n",
       "1            0.859903            0.523027            0.523671   \n",
       "2            0.841224            0.505636            0.503382   \n",
       "3            0.833494            0.493398            0.494686   \n",
       "4            0.826087            0.830596            0.827697   \n",
       "5            0.821578            0.827375            0.818680   \n",
       "6            0.813527            0.821256            0.817713   \n",
       "\n",
       "   split5_train_score  split6_train_score  split7_train_score  \\\n",
       "0            0.895330            0.895008            0.895008   \n",
       "1            0.864734            0.527536            0.861192   \n",
       "2            0.846699            0.840258            0.839936   \n",
       "3            0.836715            0.829952            0.832850   \n",
       "4            0.828986            0.820934            0.828019   \n",
       "5            0.828019            0.819002            0.820612   \n",
       "6            0.826087            0.817713            0.816747   \n",
       "\n",
       "   split8_train_score  split9_train_score  mean_train_score  std_train_score  \n",
       "0            0.892110            0.889569          0.893945         0.002184  \n",
       "1            0.859903            0.865422          0.760612         0.154428  \n",
       "2            0.840902            0.841275          0.774493         0.135009  \n",
       "3            0.832206            0.831294          0.765250         0.135620  \n",
       "4            0.826731            0.830650          0.826538         0.003276  \n",
       "5            0.820934            0.822923          0.821481         0.003500  \n",
       "6            0.815781            0.819704          0.817810         0.003597  \n",
       "\n",
       "[7 rows x 31 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view data with complete tuning results\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#10. Choose a third model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr.coef_: [ 0.25221812 -0.10849558  0.2639728 ]\n",
      "lr.intercept_: 0.2952086136487615\n"
     ]
    }
   ],
   "source": [
    "#Linear Regerssion is chosen\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X, y, random_state=622)\n",
    "lr = LinearRegression().fit(X_train_3, y_train_3)\n",
    "\n",
    "#The “slope” parameters (w), also called weights or coefficients, are stored in the coef_\n",
    "#..attribute, while the offset or intercept (b) is stored in the intercept_ attribute:\n",
    "\n",
    "print(\"lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2271624 , 0.212727  , 0.2376992 , 0.15792242, 0.15433743,\n",
       "       0.15192034, 0.0712923 , 0.17609263, 0.18833643, 0.14389318])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LinearRegression(), X_train_3, y_train_3, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.18\n",
      "Test set score: -0.24\n"
     ]
    }
   ],
   "source": [
    "# Let’s look at the training set and test set performance using r squared:\n",
    "\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train_3, y_train_3)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test_3, y_test_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>spam</td>       <th>  R-squared:         </th> <td>   0.180</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.179</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   251.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 22 Jun 2019</td> <th>  Prob (F-statistic):</th> <td>8.13e-148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:02:39</td>     <th>  Log-Likelihood:    </th> <td> -2084.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  3450</td>      <th>  AIC:               </th> <td>   4176.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  3446</td>      <th>  BIC:               </th> <td>   4201.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                 <td>    0.2952</td> <td>    0.009</td> <td>   34.658</td> <td> 0.000</td> <td>    0.279</td> <td>    0.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_business:</th>   <td>    0.2522</td> <td>    0.018</td> <td>   14.122</td> <td> 0.000</td> <td>    0.217</td> <td>    0.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_conference:</th> <td>   -0.1085</td> <td>    0.025</td> <td>   -4.346</td> <td> 0.000</td> <td>   -0.157</td> <td>   -0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>char_freq_!:</th>          <td>    0.2640</td> <td>    0.013</td> <td>   21.046</td> <td> 0.000</td> <td>    0.239</td> <td>    0.289</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>192.995</td> <th>  Durbin-Watson:     </th> <td>   1.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 129.243</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.358</td>  <th>  Prob(JB):          </th> <td>8.61e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.378</td>  <th>  Cond. No.          </th> <td>    3.52</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   spam   R-squared:                       0.180\n",
       "Model:                            OLS   Adj. R-squared:                  0.179\n",
       "Method:                 Least Squares   F-statistic:                     251.9\n",
       "Date:                Sat, 22 Jun 2019   Prob (F-statistic):          8.13e-148\n",
       "Time:                        00:02:39   Log-Likelihood:                -2084.2\n",
       "No. Observations:                3450   AIC:                             4176.\n",
       "Df Residuals:                    3446   BIC:                             4201.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=========================================================================================\n",
       "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------\n",
       "const                     0.2952      0.009     34.658      0.000       0.279       0.312\n",
       "word_freq_business:       0.2522      0.018     14.122      0.000       0.217       0.287\n",
       "word_freq_conference:    -0.1085      0.025     -4.346      0.000      -0.157      -0.060\n",
       "char_freq_!:              0.2640      0.013     21.046      0.000       0.239       0.289\n",
       "==============================================================================\n",
       "Omnibus:                      192.995   Durbin-Watson:                   1.973\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              129.243\n",
       "Skew:                           0.358   Prob(JB):                     8.61e-29\n",
       "Kurtosis:                       2.378   Cond. No.                         3.52\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_3_new = sm.add_constant(X_train_3)\n",
    "model = sm.OLS(y_train_3, X_train_3_new ).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#11. Choose a fourth model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#Random Forest is chosen\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(X, y,\n",
    "                                                random_state=623)\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "model.fit(X_train_4, y_train_4)\n",
    "y_pred_2 = model.predict(X_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78901734, 0.80635838, 0.76011561, 0.80346821, 0.79710145,\n",
       "       0.7884058 , 0.8255814 , 0.79360465, 0.81104651, 0.7877907 ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross validation\n",
    "cross_val_score(RandomForestClassifier(), X_train_4, y_train_4, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.81      0.83       726\n",
      "          1       0.70      0.77      0.73       425\n",
      "\n",
      "avg / total       0.80      0.79      0.80      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_pred_2, y_test_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_test_4, y_pred_2)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#12. Now rerun your best model from questions 8 through 11, but this time add three new variables to the model that you think will increase prediction accuracy.  Did this model predict test data better than your previous models?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is KNN as the accuracy is 0.816. The three new varaibles I select are 'word_freq_you', 'char_freq_[', and capital_run_length_total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.93\n",
      "1    3.47\n",
      "2    1.36\n",
      "3    3.18\n",
      "4    3.18\n",
      "Name: word_freq_you:, dtype: float64\n",
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "Name: char_freq_[:, dtype: float64\n",
      "0     278\n",
      "1    1028\n",
      "2    2259\n",
      "3     191\n",
      "4     191\n",
      "Name: capital_run_length_total:, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#X1=df.iloc[:, 16]   #business\n",
    "#X2=df.iloc[:, 47] #conference\n",
    "#X3=df.iloc[:,51] #CHAR_!\n",
    "\n",
    "X4=df.iloc[:, 18]\n",
    "X5=df.iloc[:, 50]\n",
    "X6=df.iloc[:,-2]\n",
    "\n",
    "print (X4.head())\n",
    "print(X5.head())\n",
    "print(X6.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_business:</th>\n",
       "      <th>word_freq_you:</th>\n",
       "      <th>word_freq_conference:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.07</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_business:  word_freq_you:  word_freq_conference:  char_freq_[:  \\\n",
       "0                 0.00            1.93                    0.0           0.0   \n",
       "1                 0.07            3.47                    0.0           0.0   \n",
       "2                 0.06            1.36                    0.0           0.0   \n",
       "3                 0.00            3.18                    0.0           0.0   \n",
       "4                 0.00            3.18                    0.0           0.0   \n",
       "\n",
       "   char_freq_!:  capital_run_length_total:  \n",
       "0         0.778                        278  \n",
       "1         0.372                       1028  \n",
       "2         0.276                       2259  \n",
       "3         0.137                        191  \n",
       "4         0.135                        191  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new=df.iloc[:,[16, 18, 47, 50, 51, -2]]\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "#k=5\n",
    "X_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(X_new, y)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_5, y_train_5)\n",
    "\n",
    "#Print accuracy rounded to two digits to the right of decimal\n",
    "print(\"accuracy: {:.2f}\".format(knn.score(X_test_5, y_test_5)))\n",
    "\n",
    "y_pred = knn.predict(X_test_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "#k=10\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train_5, y_train_5)\n",
    "\n",
    "#Print accuracy rounded to two digits to the right of decimal\n",
    "print(\"accuracy: {:.2f}\".format(knn.score(X_test_5, y_test_5)))\n",
    "\n",
    "y_pred = knn.predict(X_test_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, k=5 is selected as the parameter. And the accuracy is worse than previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#13. Rerun all your other models with this final set of six variables, evaluate prediction error, and choose a final model.  Why did you select this model among all of the models that you ran?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg .coef_: [[ 1.58745946e+00  2.89224549e-01 -5.46451853e+00 -2.61432989e+00\n",
      "   1.42931574e+00  1.50169334e-03]]\n",
      "Training set score: 0.782\n",
      "Test set score: 0.774\n",
      "logreg.predict: [0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1e+90, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "X_train_6, X_test_6, y_train_6, y_test_6 = train_test_split(X_new, y, random_state=624) \n",
    "logreg = LogisticRegression(C=1e90).fit(X_train_6, y_train_6)\n",
    "\n",
    "print(\"logreg .coef_: {}\".format(logreg .coef_))\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train_6, y_train_6)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test_6, y_test_6)))\n",
    "\n",
    "predicted_vals = logreg.predict(X_test_6) # y_pred includes your predictions\n",
    "print(\"logreg.predict: {}\".format(predicted_vals))\n",
    "logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35504886, 0.44021739, 0.83913043, 0.87173913, 0.74347826])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LogisticRegression(), X_new, y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/statsmodels/genmod/families/family.py:880: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  n_endog_mu = self._clean((1. - endog) / (1. - mu))\n",
      "/anaconda3/lib/python3.6/site-packages/statsmodels/genmod/families/family.py:932: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  special.gammaln(n - y + 1) + y * np.log(mu / (1 - mu)) +\n",
      "/anaconda3/lib/python3.6/site-packages/statsmodels/genmod/families/family.py:932: RuntimeWarning: invalid value encountered in multiply\n",
      "  special.gammaln(n - y + 1) + y * np.log(mu / (1 - mu)) +\n",
      "/anaconda3/lib/python3.6/site-packages/statsmodels/genmod/families/family.py:933: RuntimeWarning: divide by zero encountered in log\n",
      "  n * np.log(1 - mu)) * var_weights\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>spam</td>       <th>  No. Observations:  </th>  <td>  3450</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  3443</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>     6</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td>     nan</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Sat, 22 Jun 2019</td> <th>  Deviance:          </th> <td>     inf</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>00:02:42</td>     <th>  Pearson chi2:      </th> <td>4.50e+15</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>         <td>4</td>        <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>                 <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                     <td>   -1.8106</td> <td>    0.073</td> <td>  -24.925</td> <td> 0.000</td> <td>   -1.953</td> <td>   -1.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_business:</th>       <td>    1.5948</td> <td>    0.153</td> <td>   10.402</td> <td> 0.000</td> <td>    1.294</td> <td>    1.895</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_you:</th>            <td>    0.2891</td> <td>    0.024</td> <td>   12.196</td> <td> 0.000</td> <td>    0.243</td> <td>    0.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_conference:</th>     <td>   -5.9316</td> <td>    0.884</td> <td>   -6.710</td> <td> 0.000</td> <td>   -7.664</td> <td>   -4.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>char_freq_[:</th>              <td>   -2.6963</td> <td>    0.861</td> <td>   -3.133</td> <td> 0.002</td> <td>   -4.383</td> <td>   -1.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>char_freq_!:</th>              <td>    1.4287</td> <td>    0.119</td> <td>   12.047</td> <td> 0.000</td> <td>    1.196</td> <td>    1.661</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>capital_run_length_total:</th> <td>    0.0015</td> <td>    0.000</td> <td>   12.961</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                   spam   No. Observations:                 3450\n",
       "Model:                            GLM   Df Residuals:                     3443\n",
       "Model Family:                Binomial   Df Model:                            6\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                    nan\n",
       "Date:                Sat, 22 Jun 2019   Deviance:                          inf\n",
       "Time:                        00:02:42   Pearson chi2:                 4.50e+15\n",
       "No. Iterations:                     4   Covariance Type:             nonrobust\n",
       "=============================================================================================\n",
       "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------\n",
       "const                        -1.8106      0.073    -24.925      0.000      -1.953      -1.668\n",
       "word_freq_business:           1.5948      0.153     10.402      0.000       1.294       1.895\n",
       "word_freq_you:                0.2891      0.024     12.196      0.000       0.243       0.336\n",
       "word_freq_conference:        -5.9316      0.884     -6.710      0.000      -7.664      -4.199\n",
       "char_freq_[:                 -2.6963      0.861     -3.133      0.002      -4.383      -1.010\n",
       "char_freq_!:                  1.4287      0.119     12.047      0.000       1.196       1.661\n",
       "capital_run_length_total:     0.0015      0.000     12.961      0.000       0.001       0.002\n",
       "=============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_6_new = sm.add_constant(X_train_6)\n",
    "model_6 = sm.GLM(y_train_6, X_train_6_new, family=sm.families.Binomial()).fit()\n",
    "\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr.coef_: [ 2.50216452e-01  6.27437722e-02 -1.06684158e-01 -1.42953653e-01\n",
      "  8.57230655e-02  1.72601379e-04]\n",
      "lr.intercept_: 0.18458203865089523\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression\n",
    "X_train_7, X_test_7, y_train_7, y_test_7 = train_test_split(X_new, y, random_state=625)\n",
    "lr = LinearRegression().fit(X_train_7, y_train_7)\n",
    "\n",
    "#The “slope” parameters (w), also called weights or coefficients, are stored in the coef_\n",
    "#..attribute, while the offset or intercept (b) is stored in the intercept_ attribute:\n",
    "\n",
    "print(\"lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24224233,  0.25470955,  0.2350052 ,  0.20874053,  0.15091186,\n",
       "       -0.08828037,  0.20603581,  0.24542572,  0.23899355,  0.21183698])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LinearRegression(), X_train_7, y_train_7, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.22\n",
      "Test set score: 0.23\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train_7, y_train_7)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test_7, y_test_7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>spam</td>       <th>  R-squared:         </th> <td>   0.220</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.219</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   162.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 22 Jun 2019</td> <th>  Prob (F-statistic):</th> <td>5.76e-182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:02:42</td>     <th>  Log-Likelihood:    </th> <td> -1992.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  3450</td>      <th>  AIC:               </th> <td>   3998.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  3443</td>      <th>  BIC:               </th> <td>   4041.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                     <td>    0.1846</td> <td>    0.011</td> <td>   16.867</td> <td> 0.000</td> <td>    0.163</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_business:</th>       <td>    0.2502</td> <td>    0.017</td> <td>   14.784</td> <td> 0.000</td> <td>    0.217</td> <td>    0.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_you:</th>            <td>    0.0627</td> <td>    0.004</td> <td>   14.977</td> <td> 0.000</td> <td>    0.055</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>word_freq_conference:</th>     <td>   -0.1067</td> <td>    0.027</td> <td>   -3.991</td> <td> 0.000</td> <td>   -0.159</td> <td>   -0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>char_freq_[:</th>              <td>   -0.1430</td> <td>    0.065</td> <td>   -2.214</td> <td> 0.027</td> <td>   -0.270</td> <td>   -0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>char_freq_!:</th>              <td>    0.0857</td> <td>    0.008</td> <td>   10.155</td> <td> 0.000</td> <td>    0.069</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>capital_run_length_total:</th> <td>    0.0002</td> <td> 1.14e-05</td> <td>   15.191</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>30.753</td> <th>  Durbin-Watson:     </th> <td>   2.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  31.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.218</td> <th>  Prob(JB):          </th> <td>1.34e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.175</td> <th>  Cond. No.          </th> <td>6.25e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 6.25e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   spam   R-squared:                       0.220\n",
       "Model:                            OLS   Adj. R-squared:                  0.219\n",
       "Method:                 Least Squares   F-statistic:                     162.2\n",
       "Date:                Sat, 22 Jun 2019   Prob (F-statistic):          5.76e-182\n",
       "Time:                        00:02:42   Log-Likelihood:                -1992.0\n",
       "No. Observations:                3450   AIC:                             3998.\n",
       "Df Residuals:                    3443   BIC:                             4041.\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=============================================================================================\n",
       "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------\n",
       "const                         0.1846      0.011     16.867      0.000       0.163       0.206\n",
       "word_freq_business:           0.2502      0.017     14.784      0.000       0.217       0.283\n",
       "word_freq_you:                0.0627      0.004     14.977      0.000       0.055       0.071\n",
       "word_freq_conference:        -0.1067      0.027     -3.991      0.000      -0.159      -0.054\n",
       "char_freq_[:                 -0.1430      0.065     -2.214      0.027      -0.270      -0.016\n",
       "char_freq_!:                  0.0857      0.008     10.155      0.000       0.069       0.102\n",
       "capital_run_length_total:     0.0002   1.14e-05     15.191      0.000       0.000       0.000\n",
       "==============================================================================\n",
       "Omnibus:                       30.753   Durbin-Watson:                   2.019\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               31.655\n",
       "Skew:                           0.218   Prob(JB):                     1.34e-07\n",
       "Kurtosis:                       3.175   Cond. No.                     6.25e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 6.25e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_7_new = sm.add_constant(X_train_7)\n",
    "model_7 = sm.OLS(y_train_7, X_train_7_new ).fit()\n",
    "\n",
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79479769, 0.75433526, 0.80869565, 0.79710145, 0.77971014,\n",
       "       0.81449275, 0.7826087 , 0.8173913 , 0.80232558, 0.80813953])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train_8, X_test_8, y_train_8, y_test_8 = train_test_split(X, y,\n",
    "                                                random_state=626)\n",
    "cross_val_score(RandomForestClassifier(), X_train_8, y_train_8, cv=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8 = RandomForestClassifier(n_estimators=1000)\n",
    "model_8.fit(X_train_4, y_train_4)\n",
    "y_pred_8 = model.predict(X_test_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.87      0.89       727\n",
      "          1       0.79      0.86      0.82       424\n",
      "\n",
      "avg / total       0.87      0.86      0.86      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_pred_8, y_test_8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat_1 = confusion_matrix(y_test_8, y_pred_8)\n",
    "sns.heatmap(mat_1.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#14. What variable that currently is not in your model, if included, would be likely to increase your final model's predictive power?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the email address. Some spam email's address has some specific characters and I think it is worth tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#15. Lastly, you have listed each of the models that we have learned to use to predict dependent variables like spam.  List each model we have focused on in class thus far that you could use to evaluate data with a continuous dependent variable.   (Include models we have learned through class 5 only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. K-Nearest Neighborhood;\n",
    "2. Linear Regression;\n",
    "3. Support Vector Machine;\n",
    "4. Decision Tree;\n",
    "5. Random Forest;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
